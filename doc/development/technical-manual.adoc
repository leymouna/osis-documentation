= OSIS Technical Manual
Hildeberto Mendonça, PhD
v0.1, 2015-09-12
:toc: right

== Environments

=== Development

During this section, we explain in details how to prepare the development environment in a Debian-based distribution (Linux Debian, Ubuntu, Mint, etc.). We have to:

 - install and configure Git;
 - install and configure PostgreSQL;
 - Install and configure Odoo;
 - Install and configure OSIS modules; and
 - create an account on Github and learn the social aspects of it

==== Installing and Configuring Git

We start by installing Git to be able to download Odoo's and OSIS' source code:

    $ sudo apt-get update
    $ sudo apt-get install git

The `update` command downloads package lists from repositories to get information on the newest versions of packages and their dependencies. This way, we make sure we are getting the last version of Git and all other dependencies.

Then, we add some personal information in the local Git installation to make sure the author is well identified in all commits:

    $ git config --global user.name "[Firstname] [Lastname]"
    $ git config --global user.email "[firstname.lastname]@uclouvain.be"

Since version 2.0, Git has adopted a new behavior to pull and push commits while in a branch. When you execute `git push` or `git pull` Git will consider pushing or pulling just for the current branch. Before, these commands would push and pull all branches. But the change to this new behavior is voluntary, not automatically imposed. So, we have to explicitly say we have to move from the old behavior to the new one. To do that, execute the following command:

    $ git config --global push.default simple

==== Installing and Configuring PostgreSQL

PostgreSQL is the only database supported by Odoo. There is no safe way to use another database, such as MySQL or Oracle. Fortunately, PostgreSQL has a very good reputation, a large community and a generous documentation. Execute the following commands to install it:

    $ sudo apt-get install postgresql
    $ sudo su - postgres -c "createuser -s $USER"
    $ sudo apt-get install libpq-dev

The first command installs PortgreSQL and creates a database user named after the current logged OS user. The library libpq-dev is also installed for development purposes.

==== Installing Odoo for Development Purpose

The following steps describe how to install Odoo - an open source platform for business applications - from the source code. Before downloading the source, let's create a directory in the home folder (`/home/[username]`) to keep together everything related to Python development:

    $ mkdir ~/python

Then, go to the new folder and clone Odoo source code locally:

    $ cd ~/python
    $ git clone https://github.com/odoo/odoo.git -b 8.0

The command above creates a folder named `odoo` containing the source code of the main branch on the server, which is usually related to the latest stable version. At this point, Odoo is not yet ready to run. We still have to install all its dependencies. Then we continue with the installation of Python development dependencies:

    $ sudo apt-get install libxml2-dev libxslt-dev libevent-dev libsasl2-dev libldap2-dev python-dev python-setuptools python-pip python-unittest2

The basic development dependencies are installed to enable the environment to install Odoo's direct dependencies:

    $ cd ~/python/odoo
    $ sudo pip install -r requirements.txt

The file `requirements.txt` contains a list of Odoo's direct dependencies. The `pip` command allows the installation of all dependencies by passing this file by parameter.

Now, Odoo is ready to run. To test the installation, run Odoo from command line:

    $ cd ~/python/odoo
    $ ./odoo.py

To see Odoo running, visit the URL http://localhost:8069. To stop Odoo, go back to the terminal and type `Ctrl+C`. If it doesn't work, then try `Ctrl+Shift+C`.

==== Installing OSIS modules on Odoo

Before installing the new modules, we have to create a database for Odoo. It is done through Odoo's user interface. Follow the steps below:

. start Odoo and visit the address http://localhost:8069;
. go to the section "Manage Databases";
. select "Create" on the left menu;
. fill in the form:
.. inform the administration password;
.. `osis` as the name of the new database;
.. leave "Loading demonstration data" unchecked;
.. select "English" as the database language; and
.. define a secure password for the administrator.

. activate "Technical Features":
.. login as `admin` with the password you have just created for the new database;
.. select "Settings" on the top menu and "Users" on the left menu;
.. edit the "Administrator" user and go to "Access Rights" tab;
.. check the checkbox "Technical Features" and click on "Save";
.. reload the page and you will have access to many Odoo server internals.

To keep everything organized, create the following directory structure in your python folder (`/home/[username]/python`):

    $ cd ~/python
    $ mkdir -p projects/osis

Go to the new directory and clone OSIS modules:

    $ cd projects/osis
    $ git clone https://github.com/uclouvain/osis-core.git
    $ git clone https://github.com/uclouvain/osis-louvain.git

Go to Odoo's directory and create an initialization file:

    $ cd ~/python/odoo
    $ ./odoo.py --save --stop-after-init

The file `.openerp_serverrc` is created in your home directory. Edit the initialization file (.openerp_serverrc) and add the location of OSIS modules in the attribute `addons_path`:

    $ nano ~/.openerp_serverrc
           ...
           addons_path = ...,/home/[username]/python/projects/osis
           ...

Go to Odoo's folder and install the new modules:

    $ cd ~/python/odoo
    $ ./odoo.py -d osis -i osis-core,osis-louvain

==== Django

Django is a modern and lightweight web framework to support our front-end applications.

=== Contributing to OSIS

image::images/git-state-diagram.png[Git State Diagram]

The code repository is organized in three fixed branches:

- *dev*: agregates developers' contributions that are intended to be in production, but they still need to be validated.
- *qa*: at the end of the sprint, when all features are frozen, the branch `dev` is merged into `qa` to allow testers to validate the release before it gets into production.
- *master*: once the version in `qa` is fully validated, it is merged into the branch `master`, which is the one to be deployed in production.

Developers should not commit directly to any of these branches. By convention, the source code can only be changed under the context of an issue created on the issue tracker tool.

==== Creating and Working in a Branch

The issue tracker generates an incremental id that we can use to name branches. It helps to keep branches linked to issues. For example: considering an issue with the id 260, we can create a local branch with the following commands:

    $ git fetch origin dev
    $ git checkout dev
    $ git checkout -b issue#260

The first command updates the branch `dev` with the last changes on the server. The second command moves from the branch we are at the moment to the branch `dev`. The last command creates the branch `issue#260` from `dev` and immediately moves to it. From this moment, every commit will be attached to the correct branch. If the branch `dev` already exists in local, then instead of fetching it we should pull it:

    $ git pull origin dev

As we work on the issue, two commands are very useful to keep track of what has been done:

    $ git status
    $ git diff models.py

The first command shows all created, modified and removed files that are candidates to be committed. The second shows the changes in one of the modified files. When we are ready to commit, we should decide whether all changed files will be included in the commit or just a subset of them. To include all files:

    $ git commit -a -m "New entities added."

To include a subset of files, we have to add each file individually:

    $ git add models.py
    $ git add __init__.py
    $ git commit -m "New entities added."

Committing often is encouraged. All commits are done locally, thus there is no risk of conflicts until all commits are sent to the server. The `push` option sends all commits in a local branch to the server, identified by `origin`.

    $ git push origin issue#260

==== Fixing Mistakes

Version control doesn’t always happens smoothly. We will certainly face some problems and fortunately Git is very gentile on which concerns recovering from mistakes. These are some common situations we may face during development.

===== Moving to another branch before finishing the work in the current branch

Sometimes we are working in a branch and a more urgent problem arrives, requiring us to move to or create another branch. In this case, we have to commit all changes in the current branch before moving to another one, otherwise we risk to have our changes to the current branch committed in another branch. So, first add your changes and commit:

    $ git commit -a -m "New entities added but still incomplete."

and then move to an existing branch:

    $ git checkout issue#261

or create another branch from `dev`:

    $ git checkout dev
    $ git checkout -b issue#261

It also happens that we start fixing an issue but we forget to move to its respective branch. In this case, we have to commit the files related to the current branch and leave in the workspace the changes related to another branch:

    $ git add calendar.py
    $ git commit -m "Sort algorithm started."
    $ git checkout issue#260

The files that were not committed in the previous branch will be available for commit in the branch issue#260.

This practical approach of moving from a branch to another while leaving some files uncommitted may not work if at least one of the files we have changed locally was also changed remotely. We may see a message like this:

    From https://github.com/uclouvain/osis-louvain
    * branch            dev        -> FETCH_HEAD
    Updating 57c4a6d..9839a25
    error: Your local changes to the following files would be overwritten
           by merge:
           __openerp__.py
    Please, commit your changes or stash them before you can merge.
    Aborting

In this case, we do have to commit local changes before moving to the other branch.

===== Fixing the latest commit message

    $ git commit --amend -m "message"

When we work with branches it’s very common to fool with the commits. There are many branches locally and sometimes we forget to switch to the branch related to the issue
and we end up committing on the wrong branch. When it happens before pushing the commits to the server, we can undo the last commit done with the command:

    $ git reset --soft HEAD~1

But if the commit was already pushed to the server, it is still possible to undo the push as long as other people have not pushed to the same branch after the wrong push. It is done with the following command:

    $ git push origin master -f

Stop tracking a file without deleting it locally:

    $ git rm --cached [file]

== Data

=== Data Model

==== EPC

===== Databases

There are 5 EPC databases, one for each EPC instance - *dev*, *test*, *qa*, *demo* and *production*. Within each database, EPC has access to 8 schemes - *epc*, *aid*, *fgs*, *mnd*, *pres*, *std*, *str* and *doctorats*. The schema epc depends on fgs, mnd, pres, std and str. The schema aid depends on epc. The schema doctorats is isolated. These schemes are in the scope of the database migration.

===== Files

For performance reasons, files generated by EPC are stored in a network storage space. Only references for those files are kept in the database. It significantly improved performance and maintenance in comparison to storing files directly in the database, as it was done before. Documents have an expiration date which varies from 0, for temporary files, to 3 years for more relevant documents. Since no document is older than three years, historical data are not an issue.

==== OSIS

=== Data Migration

Once the decision to migrate the applications to Odoo was made, a detailed technical analysis takes place to identify the implications of this migration in order to help decision makers to define priorities and conceive a realistic planning. The current assumption is that the data is probably the only resource that will be preserved in the process of rewriting all applications on Odoo's framework. Therefore, this document focus on the data migration only.

Odoo completely abstracts the database from programmers. The database model is created using a object-relational model where classes are used to represent database tables. Objects from those classes represent data from their respective tables. The difference from the current architecture is that programmers are fully responsible for creating the physical model while Odoo takes full responsibility over this model. Therefore, there is a very low probability that the current data model is anyhow compatible with data models managed by Odoo.

A clear evidence of that is the approach adopted by Odoo to define primary keys. While it always define a unique, numeric, auto-incremented identifier, the current physical model uses all sorts of approaches such as: single numeric column, single character column, multiple numeric columns, multiple heterogeneous columns and others. Therefore, preserving the referential integrity of the data is probably the most challenging issue to be addressed in this analysis.
This document aims to support the decision making of the project manager by gathering technical information about the data, analyzing the implications and proposing solutions for the identified issues.

==== Strategies

The main issue identified in the previous section is how to preserve referential integrity when the data is spread in different database servers. We will probably never find a 100% reliable solution given the complexity of distributed systems, but we can considerably reduce the risk of data inconsistencies by carefully evaluating all possible alternatives and picking the one with the best cost-benefit. This is indeed an effort that cannot be postponed neither avoided. We have figured 5 migration strategies, as described hereafter.

===== Synchronize data using a synchronization tool

A off-the-shelf product is used to synchronize data between Oracle and PostgreSQL bidirectionally. This solution considers that the data model is identical or very similar in both databases. This strategy is very unlikely because EPC's data model do not follow standard rules, while Odoo follows strict rules enforced by its persistence mechanism. These discrepancies may force the implementation of very specific migration logic, which is not usually covered by migration tools.

image::images/ots-sync-tool.png[]

===== Write a program to migrate data from Oracle to PostgreSQL

It seems to be inevitable the development of a custom migration tool to address this particular data migration scenario. Therefore, all the following strategies consider some level of additional development. This one, in particular, considers the development of a tool that is scheduled to run periodically, calculating the delta between both databases and updating the most out dated one.

image::images/st-sync-tool.png[]

The data model can be different because the tool encapsulates all data transformations between the models. The data model can evolve and solve current issues.

It might be more complex and more time consuming and, since it does not use the business layer to process the data, it can become inconsistent over time if the tool does not follow carefully all changes in the business layer (i.e. boundaries of transactional business operations on multiple tables can guarantee consistency while unbounded transactions made by a synchronization tool may fail, causing inconsistency).

===== Change both applications to access each other's web services

This strategy address the disadvantages of the previous one by forcing the use of the business layer during the data migration. It is possible because all updates are done through web services that processes the data in the business layer before persisting then in the database.

image::images/wsc-sync-tool.png[]

The disadvantage is that it makes EPC and Odoo highly coupled because it forces both applications to be aware of each other. As a consequence, a locoincide comt of code would have to be removed from Odoo after the complete phase out of EPC. This is a hard task because we it is not easy to distinguish which code is concrete and which one is volatile.

===== Change one of the applications to access other's web services

We could reduce high coupling by concentrating all changes for data migration on the EPC side. This way, the migration code would be discarded with EPC, leaving Odoo free of volatile code. EPC would call Odoo's web services to update its own data for every table owned by Odoo. These  data would be available read-only on EPC.

image::images/wsc-st-sync-tool.png[]

Unfortunately, an additional tool would be necessary to keep Odoo up to date with data from those tables that are still owned by EPC.

===== Post on a queue every time an update in the database occurs

This is probability the strongest strategy because it addresses all previous drawbacks. Every update on tables not yet owned by Odoo would cause a post of a message in a queue. Messages in this queue would be read by a tool, which would call Odoo web services to pass through the business layer before updating the database.

It is feasible because the business layer in EPC is implemented using EJBs and an interceptor can be attached to a EJB to have access to the data passed as arguments and returned to the caller. An interceptor would be responsible for posting on the queue.  This way, every update done by EPC is immediately available on Odoo's data model on demand.

image::images/queue-sync-tool.png[]

To identify potential drawbacks, it would be necessary to implement a proof-of-concept in order to address unforeseen issues before starting the migration to Odoo.

=== Historical Data

The current database stores data since 1984, which matches with the beginning of information systems adoption. These data are preserved, but most of them are not useful anymore for current operational processes. They actually contribute to slow down the application by constantly increasing the size of the indexes.

Historical data cannot be simply ignored in a completely new application because the nature of EPC's data is historical by default. For example, data related to students should be available from the oldest active student until the newest one, making the studies history always available for regular reporting and updates. The period in which historical data are useful might be large, but more than 30 years of historical data certainly exceeds any reasonable limit.

The challenge is to differentiate useful historical data from archivable ones. We start by classifying EPC data in four categories:

1. *Master data*:  related to the core business but treated individually, outside of a process context. For example: offers, activities, courses, etc.

2. *Business process data*: related to business processes, such as deliberation, registrations, activities approval, encodage des notes, etc.

3. *Reference data*: not directly related to the business, but related to the education domain, complementing master data. For example: countries, languages, postal codes, etc.

4. *Auditing data*: every time a record changes a version of it is preserved in an auditing table for possible data recovery.

The data within those categories can be:

1. *Operational*: data frequently updated and retrieved from the database for on-line use or reporting. All categories above contains operational data.

2. *Archivable*: data that are not used anymore in the current business context, unless for some historical reports. Business process and auditing data are strong candidates for archiving. Master and reference data are usually required for a longer period of time and should be analyzed case by case.
